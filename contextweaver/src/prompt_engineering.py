# src/prompt_engineering.py
from typing import Dict, List, Optional, Tuple
import logging
from langchain_core.documents import Document

from config import Config

logging.basicConfig(level=Config.LOG_LEVEL)
logger = logging.getLogger(__name__)


class PromptEngineeringSystem:
    """
    Comprehensive prompt engineering system
    
    Requirements:
    1. Design systematic prompting strategies ‚úÖ
    2. Implement context management ‚úÖ
    3. Create specialized user interaction flows ‚úÖ
    4. Handle edge cases and errors gracefully ‚úÖ
    
    Features:
    - Multiple prompt templates for different tasks
    - Few-shot learning with examples
    - Chain-of-thought reasoning
    - Context window management
    - Error recovery prompts
    """
    
    def __init__(self):
        self.prompt_templates = self._initialize_templates()
        self.few_shot_examples = self._load_few_shot_examples()
        self.context_manager = ContextManager()
        logger.info("‚úÖ Prompt engineering system initialized")
    
    def _initialize_templates(self) -> Dict[str, str]:
        """
        Initialize systematic prompt templates
        
        Requirement: "Design systematic prompting strategies"
        
        8 specialized templates for different reasoning tasks
        """
        
        return {
            # Template 1: Multi-document reasoning
            'multi_document_reasoning': """You are an advanced multi-document reasoning system.

**Task:** {task}

**Available Documents:**
{documents}

**Instructions:**
Follow this systematic reasoning process:
1. **Document Analysis**: Identify key claims from each document
2. **Cross-Reference**: Find connections between documents
3. **Evidence Chain**: Build logical connections across sources
4. **Synthesis**: Create coherent answer with proper citations

**Query:** {query}

**Your Step-by-Step Analysis:**""",

            # Template 2: Contradiction detection
            'contradiction_detection': """You are a contradiction detection specialist.

**Task:** Identify contradictory claims across multiple documents.

**Documents to Analyze:**
{documents}

**Detection Process:**
1. Extract all factual claims from each document
2. Compare claims for contradictions
3. For each contradiction found:
   - Quote both conflicting statements
   - Explain WHY they contradict
   - Rate severity: LOW / MEDIUM / HIGH
   - Provide confidence score: 0.0 to 1.0
4. Attempt to explain the contradiction (different methodology, time period, scope)

**Output Format (JSON):**
{{
  "contradictions": [
    {{
      "claim_A": "exact quote from document A",
      "source_A": "document A name",
      "claim_B": "exact quote from document B",
      "source_B": "document B name",
      "explanation": "why these contradict",
      "severity": "HIGH|MEDIUM|LOW",
      "confidence": 0.0-1.0
    }}
  ],
  "resolution": "explanation of how to resolve contradictions"
}}

**Your Analysis:**""",

            # Template 3: Multi-hop reasoning
            'multi_hop_reasoning': """You are a multi-hop reasoning expert.

**Query:** {query}

**Available Documents:**
{documents}

**Multi-Hop Reasoning Process:**

**Step 1 - Document Selection:**
Which documents contain information relevant to the query?

**Step 2 - Information Extraction:**
What key facts can we extract from each selected document?

**Step 3 - Multi-Hop Connections:**
How do facts from different documents connect to answer the query?

**Step 4 - Reasoning Chain:**
Build the complete logical chain from documents to answer.

**Step 5 - Final Answer:**
Provide synthesized answer with inline citations.

**Format:**
REASONING CHAIN:
[Your step-by-step reasoning with document references]

ANSWER:
[Final answer with citations like [Doc1], [Doc2]]

CONFIDENCE: [0.0-1.0]

**Begin Reasoning:**""",

            # Template 4: Temporal analysis
            'temporal_analysis': """You are a temporal reasoning specialist.

**Documents from Different Time Periods:**
{documents}

**Query:** {query}

**Temporal Analysis Framework:**

1. **Timeline Construction**
   - Order information chronologically
   - Note publication dates and context

2. **Evolution Tracking**
   - How has understanding changed over time?
   - What new information emerged in later documents?

3. **Turning Points**
   - Identify key studies/events that shifted consensus
   - Note methodological improvements

4. **Current State**
   - What's the latest/most recent understanding?
   - Which sources are most current?

5. **Trend Prediction** (optional)
   - Based on the evolution, where might knowledge go next?

**Your Temporal Analysis:**""",

            # Template 5: Source credibility assessment
            'credibility_assessment': """You are a source credibility evaluator.

**Sources to Assess:**
{sources}

**Evaluation Criteria:**

For each source, evaluate:

1. **Source Type**
   - Peer-reviewed journal?
   - Preprint?
   - Official documentation?
   - Blog/article?

2. **Authority**
   - Author credentials
   - Institutional backing
   - Journal reputation (if applicable)

3. **Recency**
   - Publication date
   - Relevance to current knowledge

4. **Methodology** (if research)
   - Study design quality
   - Sample size
   - Controls for confounders

5. **Bias Risk**
   - Funding sources
   - Conflicts of interest
   - Balanced presentation

**Credibility Score:** Assign 0.0-1.0 for each source

**Your Assessment:**""",

            # Template 6: Synthesis with citations
            'synthesis_with_citations': """You are a research synthesis expert.

**Research Question:** {query}

**Source Documents:**
{documents}

**Synthesis Guidelines:**

1. **Integrate Information**
   - Combine insights from all relevant sources
   - Identify common themes and unique contributions

2. **Proper Citation**
   - Use inline citations: [Source Name, Year]
   - Every claim must be cited
   - Multiple sources for convergent evidence

3. **Acknowledge Gaps**
   - Note what is unknown or uncertain
   - Identify conflicting evidence

4. **Balanced Perspective**
   - Present multiple viewpoints if they exist
   - Don't cherry-pick evidence

**Your Synthesis:**""",

            # Template 7: Question answering with evidence
            'qa_with_evidence': """You are a precise question-answering system.

**Question:** {query}

**Available Evidence:**
{documents}

**Answer Framework:**

1. **Direct Answer** (1-2 sentences)
   - Answer the question directly first

2. **Supporting Evidence** (3-5 sentences)
   - Cite specific evidence from documents
   - Use format: "According to [Source], ..."

3. **Caveats/Limitations** (1-2 sentences)
   - Note any uncertainties or limitations

4. **Confidence Level**
   - HIGH: Strong evidence from multiple sources
   - MEDIUM: Evidence present but limited
   - LOW: Weak or contradictory evidence

**Your Response:**""",

            # Template 8: Error recovery
            'error_recovery': """**System Recovery Mode**

**Previous Error:** {error}

**Context:** {context}

**Recovery Actions:**

1. **Error Identification**
   - What specifically went wrong?
   - Was it data issue, logic issue, or missing information?

2. **Corrected Approach**
   - How should we handle this differently?
   - What alternative strategies can we use?

3. **Data Requirements**
   - If data is insufficient, what's needed?
   - Can we work with available information?

4. **Alternative Paths**
   - What other ways can we answer the query?
   - What partial answers can we provide?

**Your Recovery Response:**"""
        }
    
    def _load_few_shot_examples(self) -> Dict[str, List[Dict]]:
        """
        Load few-shot examples for different tasks
        
        Requirement: "Design systematic prompting strategies"
        
        Few-shot learning improves model performance by showing examples
        """
        
        return {
            'multi_hop': [
                {
                    'query': 'Can the CEO approve this $3M acquisition?',
                    'documents': [
                        'Contract ¬ß4.2: "CEO has authority for acquisitions under $5M"',
                        'Amendment B (2022): "CEO authority reduced to $2M per board resolution"',
                        'Bylaws Article VII: "All acquisitions >$1M require CFO co-signature"'
                    ],
                    'reasoning': """
REASONING CHAIN:
Step 1: Contract gives CEO $5M authority
Step 2: BUT Amendment B (2022) REDUCES this to $2M (supersedes contract)
Step 3: AND Bylaws require CFO approval for >$1M
Step 4: $3M exceeds both the amended CEO limit ($2M) AND requires CFO approval

ANSWER:
No, the CEO cannot unilaterally approve a $3M acquisition. Per Amendment B (2022), CEO authority is limited to $2M [Amendment B]. Additionally, the Bylaws require CFO co-signature for any acquisition exceeding $1M [Bylaws Article VII]. Therefore, this $3M acquisition requires both CEO and CFO approval.

CONFIDENCE: 0.95""",
                },
                {
                    'query': 'Is moderate coffee consumption safe for heart health?',
                    'documents': [
                        'Study 2018: High coffee (>5 cups) associated with 23% increased risk',
                        'Study 2023: Moderate coffee (2-3 cups) shows 15% risk reduction with proper controls',
                        'Meta-analysis 2022: Contradiction explained by confounding variables in earlier studies'
                    ],
                    'reasoning': """
REASONING CHAIN:
Step 1: 2018 study found increased risk for HIGH consumption
Step 2: 2023 study found PROTECTIVE effects for MODERATE consumption
Step 3: Meta-analysis explains: earlier studies lacked controls for confounders
Step 4: When confounders controlled (2023), moderate intake shows benefits

ANSWER:
Yes, moderate coffee consumption (2-3 cups/day) appears safe and potentially beneficial for heart health. Recent research with rigorous controls found a 15% reduction in cardiovascular disease risk [Study 2023]. Earlier concerns from 2018 studies were likely due to uncontrolled confounding variables like smoking and sugar intake [Meta-analysis 2022]. The key distinction is MODERATE vs HIGH consumption.

CONFIDENCE: 0.85"""
                }
            ],
            
            'contradiction': [
                {
                    'documents': [
                        'Document A (2018): "Coffee increases cardiovascular risk by 23%"',
                        'Document B (2023): "Moderate coffee shows protective cardiovascular effects"'
                    ],
                    'analysis': {
                        'contradictions': [
                            {
                                'claim_A': 'Coffee increases cardiovascular risk by 23%',
                                'source_A': 'Document A (2018)',
                                'claim_B': 'Moderate coffee shows protective cardiovascular effects',
                                'source_B': 'Document B (2023)',
                                'explanation': 'Document A studied HIGH consumption without controlling for confounders. Document B studied MODERATE consumption with rigorous controls for smoking, diet, and exercise.',
                                'severity': 'HIGH',
                                'confidence': 0.9
                            }
                        ],
                        'resolution': 'The contradiction is resolved by noting: (1) different consumption levels studied (high vs moderate), and (2) improved methodology in later study controlling for confounding variables. Current evidence supports moderate consumption as safe.'
                    }
                }
            ],
            
            'temporal': [
                {
                    'query': 'How has understanding of coffee and heart health evolved?',
                    'documents': [
                        '2010-2018: Early studies showed negative effects',
                        '2019-2021: Mixed results, methodological concerns raised',
                        '2022-2023: Studies with better controls show neutral or positive effects'
                    ],
                    'analysis': """
TIMELINE:
2010-2018: Negative associations found, but studies lacked confounder controls
2019-2021: Transition period, meta-analyses identified methodological issues
2022-2023: Improved studies with proper controls show protective effects

EVOLUTION:
Understanding shifted from "coffee is harmful" to "moderate coffee is safe/beneficial"

TURNING POINTS:
- 2022 Meta-analysis explaining confounding variables
- 2023 Study with rigorous lifestyle controls

CURRENT STATE:
Moderate coffee consumption (2-3 cups/day) considered safe or beneficial when confounders controlled"""
                }
            ]
        }
    
    def create_prompt(
        self,
        template_name: str,
        variables: Dict[str, str],
        use_few_shot: bool = Config.USE_FEW_SHOT,
        num_examples: int = Config.NUM_FEW_SHOT_EXAMPLES,
        use_chain_of_thought: bool = Config.USE_CHAIN_OF_THOUGHT
    ) -> str:
        """
        Create a prompt from template with optional enhancements
        
        Args:
            template_name: Which template to use
            variables: Variables to fill in template
            use_few_shot: Include few-shot examples
            num_examples: Number of examples to include
            use_chain_of_thought: Enable step-by-step reasoning
        """
        
        if template_name not in self.prompt_templates:
            raise ValueError(f"Unknown template: {template_name}")
        
        prompt = self.prompt_templates[template_name]
        
        # Add few-shot examples if requested
        if use_few_shot and template_name in self.few_shot_examples:
            examples = self.few_shot_examples[template_name][:num_examples]
            few_shot_text = self._format_few_shot_examples(examples, template_name)
            prompt = f"{few_shot_text}\n\n{'='*60}\n\nNow apply this reasoning to the actual query:\n\n{prompt}"
        
        # Add chain-of-thought instructions if requested
        if use_chain_of_thought and 'reasoning' in template_name.lower():
            cot_instruction = "\n\n**Remember: Think step-by-step and show your reasoning process clearly.**\n"
            prompt = prompt + cot_instruction
        
        # Fill in variables
        try:
            prompt = prompt.format(**variables)
        except KeyError as e:
            raise ValueError(f"Missing required variable in prompt: {e}")
        
        return prompt
    
    def _format_few_shot_examples(self, examples: List[Dict], template_name: str) -> str:
        """Format few-shot examples for inclusion in prompts"""
        formatted = "**Few-Shot Examples:**\n\nHere are examples of correct reasoning:\n\n"
        
        for i, example in enumerate(examples, 1):
            formatted += f"{'='*60}\n"
            formatted += f"EXAMPLE {i}:\n"
            formatted += f"{'='*60}\n\n"
            
            if 'query' in example:
                formatted += f"**Query:** {example['query']}\n\n"
            
            if 'documents' in example:
                formatted += f"**Documents:**\n"
                for j, doc in enumerate(example['documents'], 1):
                    formatted += f"{j}. {doc}\n"
                formatted += "\n"
            
            if 'reasoning' in example:
                formatted += f"**Reasoning:**\n{example['reasoning']}\n\n"
            
            if 'analysis' in example:
                import json
                formatted += f"**Analysis:**\n{json.dumps(example['analysis'], indent=2)}\n\n"
        
        return formatted
    
    def format_documents_for_prompt(
        self,
        documents: List[Document],
        include_metadata: bool = True,
        max_length: int = 500
    ) -> str:
        """
        Format documents for inclusion in prompts
        
        Requirement: "Implement context management"
        """
        formatted = ""
        
        for i, doc in enumerate(documents, 1):
            formatted += f"\n{'‚îÄ'*60}\n"
            formatted += f"DOCUMENT {i}:\n"
            formatted += f"{'‚îÄ'*60}\n"
            
            if include_metadata:
                # Add important metadata
                source = doc.metadata.get('filename', doc.metadata.get('source', 'Unknown'))
                year = doc.metadata.get('year', 'N/A')
                domain = doc.metadata.get('domain', 'N/A')
                
                formatted += f"Source: {source}\n"
                formatted += f"Year: {year}\n"
                formatted += f"Domain: {domain}\n"
                formatted += f"{'‚îÄ'*60}\n"
            
            # Add content (truncate if too long)
            content = doc.page_content
            if len(content) > max_length:
                content = content[:max_length] + "...[truncated]"
            
            formatted += f"{content}\n"
        
        return formatted
    
    def create_user_interaction_flow(
        self,
        query: str,
        query_type: str,
        documents: List[Document]
    ) -> Dict[str, any]:
        """
        Create specialized user interaction flow
        
        Requirement: "Create specialized user interaction flows"
        
        Different query types get different handling:
        - simple_qa: Direct answer with evidence
        - multi_hop: Complex multi-document reasoning
        - contradiction: Detect and explain conflicts
        - temporal: Analyze evolution over time
        - synthesis: Comprehensive research synthesis
        """
        
        flows = {
            'simple_qa': {
                'template': 'qa_with_evidence',
                'use_few_shot': False,
                'max_docs': 3,
                'expected_length': 'short'
            },
            'multi_hop': {
                'template': 'multi_hop_reasoning',
                'use_few_shot': True,
                'max_docs': 5,
                'expected_length': 'medium'
            },
            'contradiction': {
                'template': 'contradiction_detection',
                'use_few_shot': True,
                'max_docs': 10,
                'expected_length': 'long'
            },
            'temporal': {
                'template': 'temporal_analysis',
                'use_few_shot': True,
                'max_docs': 10,
                'expected_length': 'long'
            },
            'synthesis': {
                'template': 'synthesis_with_citations',
                'use_few_shot': False,
                'max_docs': 10,
                'expected_length': 'long'
            }
        }
        
        if query_type not in flows:
            query_type = 'simple_qa'  # Default
        
        flow = flows[query_type]
        
        # Prepare documents based on flow requirements
        max_docs = flow['max_docs']
        selected_docs = documents[:max_docs]
        
        # Create prompt
        formatted_docs = self.format_documents_for_prompt(selected_docs)
        
        variables = {
            'query': query,
            'documents': formatted_docs,
            'task': self._get_task_description(query_type)
        }
        
        prompt = self.create_prompt(
            template_name=flow['template'],
            variables=variables,
            use_few_shot=flow['use_few_shot']
        )
        
        return {
            'query_type': query_type,
            'prompt': prompt,
            'num_documents': len(selected_docs),
            'template_used': flow['template'],
            'few_shot_enabled': flow['use_few_shot'],
            'expected_length': flow['expected_length']
        }
    
    def _get_task_description(self, query_type: str) -> str:
        """Get task description for each query type"""
        descriptions = {
            'simple_qa': 'Answer the question directly using the provided documents',
            'multi_hop': 'Reason across multiple documents to build a complete answer',
            'contradiction': 'Identify and explain contradictions in the documents',
            'temporal': 'Analyze how understanding has evolved over time',
            'synthesis': 'Synthesize information from all sources into coherent summary'
        }
        return descriptions.get(query_type, 'Process the query using available documents')
    
    def handle_error_recovery(
        self,
        error_message: str,
        original_query: str,
        attempted_approach: str
    ) -> str:
        """
        Handle edge cases and errors gracefully
        
        Requirement: "Handle edge cases and errors gracefully"
        """
        
        variables = {
            'error': error_message,
            'context': f"Original Query: {original_query}\nAttempted Approach: {attempted_approach}"
        }
        
        recovery_prompt = self.create_prompt(
            template_name='error_recovery',
            variables=variables,
            use_few_shot=False
        )
        
        return recovery_prompt
    
    def classify_query_type(self, query: str) -> str:
        """
        Classify query to determine appropriate interaction flow
        
        Uses keyword matching to classify query type
        """
        query_lower = query.lower()
        
        # Contradiction detection keywords
        if any(word in query_lower for word in ['contradict', 'conflict', 'disagree', 'inconsistent']):
            return 'contradiction'
        
        # Temporal analysis keywords
        if any(word in query_lower for word in ['evolve', 'change', 'over time', 'history', 'timeline']):
            return 'temporal'
        
        # Multi-hop reasoning keywords
        if any(word in query_lower for word in ['why', 'explain', 'how does', 'relationship between']):
            return 'multi_hop'
        
        # Synthesis keywords
        if any(word in query_lower for word in ['summarize', 'synthesize', 'overall', 'comprehensive']):
            return 'synthesis'
        
        # Default: simple Q&A
        return 'simple_qa'


class ContextManager:
    """
    Manages context window for optimal prompt construction
    
    Requirement: "Implement context management"
    
    Features:
    - Token counting and management
    - Context prioritization
    - Sliding window for long contexts
    - Context compression
    """
    
    def __init__(self):
        self.max_context_length = Config.MAX_CONTEXT_LENGTH
        self.reserved_for_response = Config.RESERVED_TOKENS_FOR_RESPONSE
        self.available_for_context = self.max_context_length - self.reserved_for_response
        logger.info(f"üìè Context manager initialized (max: {self.max_context_length} tokens)")
    
    def manage_context(
        self,
        documents: List[Document],
        query: str,
        prompt_template: str
    ) -> Tuple[List[Document], str]:
        """
        Manage context to fit within token limits
        
        Returns:
            - Filtered documents that fit in context
            - Warning message if documents were truncated
        """
        
        # Estimate token counts (rough approximation: 1 token ‚âà 4 characters)
        query_tokens = len(query) // 4
        template_tokens = len(prompt_template) // 4
        
        available_for_docs = self.available_for_context - query_tokens - template_tokens
        
        # Select documents that fit
        selected_docs = []
        total_tokens = 0
        truncated = False
        
        for doc in documents:
            doc_tokens = len(doc.page_content) // 4
            
            if total_tokens + doc_tokens <= available_for_docs:
                selected_docs.append(doc)
                total_tokens += doc_tokens
            else:
                truncated = True
                break
        
        warning = ""
        if truncated:
            warning = f"‚ö†Ô∏è Context truncated: Using {len(selected_docs)}/{len(documents)} documents to fit token limit"
            logger.warning(warning)
        
        return selected_docs, warning
    
    def prioritize_documents(
        self,
        documents: List[Document],
        query: str
    ) -> List[Document]:
        """
        Prioritize documents for context inclusion
        
        Priority factors:
        1. Relevance (similarity score)
        2. Recency
        3. Credibility
        4. Completeness (prefer full documents over fragments)
        """
        
        scored_docs = []
        
        for doc in documents:
            priority_score = (
                doc.metadata.get('similarity_score', 0.5) * 0.4 +  # Relevance
                doc.metadata.get('credibility_score', 0.5) * 0.3 +  # Credibility
                self._calculate_recency_score(doc) * 0.2 +           # Recency
                self._calculate_completeness_score(doc) * 0.1        # Completeness
            )
            
            scored_docs.append((doc, priority_score))
        
        # Sort by priority score
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, score in scored_docs]
    
    def _calculate_recency_score(self, doc: Document) -> float:
        """Calculate recency score"""
        from datetime import datetime
        import math
        
        year = doc.metadata.get('year')
        if not year:
            return 0.5
        
        current_year = datetime.now().year
        years_old = current_year - year
        
        return math.exp(-0.1 * years_old)
    
    def _calculate_completeness_score(self, doc: Document) -> float:
        """Score based on document completeness"""
        chunk_id = doc.metadata.get('chunk_id', 0)
        total_chunks = doc.metadata.get('total_chunks', 1)
        
        # Prefer earlier chunks (more context) and complete documents
        if total_chunks == 1:
            return 1.0  # Complete document
        
        # Prefer chunks with more context (earlier chunks)
        return 1.0 - (chunk_id / total_chunks) * 0.3
    
    def compress_context(
        self,
        documents: List[Document],
        query: str,
        target_length: int
    ) -> List[Document]:
        """
        Compress context by extracting only query-relevant portions
        
        This is a simplified version - in production would use LLM
        """
        
        compressed = []
        
        for doc in documents:
            # Extract sentences containing query keywords
            query_terms = set(query.lower().split())
            sentences = doc.page_content.split('. ')
            
            relevant_sentences = []
            for sentence in sentences:
                sentence_terms = set(sentence.lower().split())
                if query_terms & sentence_terms:  # Intersection
                    relevant_sentences.append(sentence)
            
            if relevant_sentences:
                compressed_content = '. '.join(relevant_sentences)
                compressed_doc = Document(
                    page_content=compressed_content,
                    metadata={**doc.metadata, 'compressed': True}
                )
                compressed.append(compressed_doc)
        
        return compressed


class EdgeCaseHandler:
    """
    Handles edge cases and errors gracefully
    
    Requirement: "Handle edge cases and errors gracefully"
    
    Edge cases:
    - No documents found
    - Contradictory information
    - Insufficient information
    - Query out of scope
    - Malformed queries
    """
    
    def __init__(self, prompt_system: PromptEngineeringSystem):
        self.prompt_system = prompt_system
        logger.info("üõ°Ô∏è Edge case handler initialized")
    
    def handle_no_documents(self, query: str) -> Dict[str, any]:
        """Handle case when no relevant documents found"""
        return {
            'status': 'no_documents',
            'message': f"No relevant documents found for query: '{query}'",
            'suggestion': "Try rephrasing your query or adding more documents to the knowledge base.",
            'response': f"I couldn't find any documents relevant to '{query}' in the current knowledge base. Please try:\n1. Rephrasing your question\n2. Adding more documents on this topic\n3. Broadening your query"
        }
    
    def handle_insufficient_information(
        self,
        query: str,
        documents: List[Document]
    ) -> Dict[str, any]:
        """Handle case when documents don't contain enough information"""
        return {
            'status': 'insufficient_info',
            'message': f"Found {len(documents)} documents but insufficient information to answer completely",
            'partial_answer': self._generate_partial_answer(documents),
            'missing_info': self._identify_missing_information(query, documents),
            'suggestion': "The available documents provide some relevant information but may not fully answer your question."
        }
    
    def handle_contradictions(
        self,
        query: str,
        documents: List[Document]
    ) -> Dict[str, any]:
        """Handle contradictory information in documents"""
        
        # Use contradiction detection prompt
        formatted_docs = self.prompt_system.format_documents_for_prompt(documents)
        
        prompt = self.prompt_system.create_prompt(
            template_name='contradiction_detection',
            variables={'documents': formatted_docs},
            use_few_shot=True
        )
        
        return {
            'status': 'contradictions_found',
            'message': 'Contradictory information detected in documents',
            'analysis_prompt': prompt,
            'num_documents': len(documents),
            'recommendation': 'Review the contradiction analysis to understand different perspectives'
        }
    
    def handle_out_of_scope(self, query: str) -> Dict[str, any]:
        """Handle queries outside knowledge base scope"""
        return {
            'status': 'out_of_scope',
            'message': f"Query appears to be outside the knowledge base scope",
            'query': query,
            'response': f"This question appears to be outside the current knowledge base scope. The available documents focus on specific topics that may not cover '{query}'."
        }
    
    def handle_malformed_query(self, query: str, error: str) -> Dict[str, any]:
        """Handle malformed or unclear queries"""
        return {
            'status': 'malformed_query',
            'message': 'Query could not be processed',
            'error': error,
            'suggestion': 'Please rephrase your question more clearly',
            'response': f"I had trouble understanding your question. Could you please rephrase it? Error: {error}"
        }
    
    def _generate_partial_answer(self, documents: List[Document]) -> str:
        """Generate partial answer from available documents"""
        if not documents:
            return "No information available"
        
        # Simple approach: return most relevant document snippet
        return f"Based on available documents: {documents[0].page_content[:200]}..."
    
    def _identify_missing_information(
        self,
        query: str,
        documents: List[Document]
    ) -> str:
        """Identify what information is missing to answer query"""
        # Simplified implementation
        query_terms = set(query.lower().split())
        doc_terms = set()
        
        for doc in documents:
            doc_terms.update(doc.page_content.lower().split())
        
        missing_terms = query_terms - doc_terms
        
        if missing_terms:
            return f"Documents don't contain information about: {', '.join(list(missing_terms)[:5])}"
        
        return "Documents may lack specific details needed to fully answer the query"
    
    def validate_and_handle(
        self,
        query: str,
        documents: List[Document]
    ) -> Tuple[bool, Optional[Dict]]:
        """
        Validate query and documents, handle edge cases
        
        Returns:
            (is_valid, error_response_if_invalid)
        """
        
        # Edge case 1: Empty query
        if not query or len(query.strip()) < 3:
            return False, self.handle_malformed_query(query, "Query too short or empty")
        
        # Edge case 2: No documents
        if not documents:
            return False, self.handle_no_documents(query)
        
        # Edge case 3: Very low similarity scores (all documents irrelevant)
        if all(doc.metadata.get('similarity_score', 1.0) < 0.3 for doc in documents):
            return False, self.handle_insufficient_information(query, documents)
        
        # Edge case 4: Query too long
        if len(query) > 500:
            return False, self.handle_malformed_query(query, "Query too long (max 500 characters)")
        
        # All validations passed
        return True, None


# Test the prompt engineering system
if __name__ == "__main__":
    from config import Config
    
    print("=" * 60)
    print("üß™ TESTING PROMPT ENGINEERING SYSTEM")
    print("=" * 60)
    
    # Initialize system
    prompt_system = PromptEngineeringSystem()
    
    # Test 1: Create simple prompt
    print("\n1Ô∏è‚É£ Testing Basic Prompt Creation...")
    
    sample_docs = [
        Document(
            page_content="Coffee consumption linked to heart risk in 2018 study.",
            metadata={'source': 'study_2018.pdf', 'year': 2018}
        ),
        Document(
            page_content="2023 research shows coffee protective for heart health.",
            metadata={'source': 'study_2023.pdf', 'year': 2023}
        )
    ]
    
    formatted_docs = prompt_system.format_documents_for_prompt(sample_docs)
    
    variables = {
        'query': 'Is coffee good for heart health?',
        'documents': formatted_docs,
        'task': 'Answer using provided documents'
    }
    
    prompt = prompt_system.create_prompt(
        template_name='qa_with_evidence',
        variables=variables,
        use_few_shot=False
    )
    
    print(f"   ‚úÖ Created prompt ({len(prompt)} chars)")
    print(f"   Preview: {prompt[:200]}...")
    
    # Test 2: Few-shot learning
    print("\n2Ô∏è‚É£ Testing Few-Shot Learning...")
    
    prompt_with_examples = prompt_system.create_prompt(
        template_name='multi_hop_reasoning',
        variables=variables,
        use_few_shot=True,
        num_examples=1
    )
    
    print(f"   ‚úÖ Created few-shot prompt ({len(prompt_with_examples)} chars)")
    
    # Test 3: User interaction flow
    print("\n3Ô∏è‚É£ Testing User Interaction Flows...")
    
    query = "Why do studies contradict each other about coffee?"
    query_type = prompt_system.classify_query_type(query)
    print(f"   Query: '{query}'")
    print(f"   Classified as: {query_type}")
    
    flow = prompt_system.create_user_interaction_flow(query, query_type, sample_docs)
    print(f"   ‚úÖ Created {flow['query_type']} flow")
    print(f"   Template used: {flow['template_used']}")
    print(f"   Few-shot enabled: {flow['few_shot_enabled']}")
    print(f"   Documents included: {flow['num_documents']}")
    
    # Test 4: Context management
    print("\n4Ô∏è‚É£ Testing Context Management...")
    
    context_mgr = ContextManager()
    
    # Create many documents to test truncation
    many_docs = [
        Document(page_content="Document content " * 200, metadata={'source': f'doc_{i}.txt'})
        for i in range(20)
    ]
    
    selected, warning = context_mgr.manage_context(many_docs, query, prompt)
    print(f"   Original documents: {len(many_docs)}")
    print(f"   Selected to fit context: {len(selected)}")
    if warning:
        print(f"   {warning}")
    
    # Test 5: Edge case handling
    print("\n5Ô∏è‚É£ Testing Edge Case Handling...")
    
    edge_handler = EdgeCaseHandler(prompt_system)
    
    # Test no documents
    is_valid, response = edge_handler.validate_and_handle("test query", [])
    print(f"   No documents case: {response['status']}")
    
    # Test empty query
    is_valid, response = edge_handler.validate_and_handle("", sample_docs)
    print(f"   Empty query case: {response['status']}")
    
    # Test valid query
    is_valid, response = edge_handler.validate_and_handle("Is coffee healthy?", sample_docs)
    print(f"   Valid query: {'‚úÖ Passed' if is_valid else '‚ùå Failed'}")
    
    print("\n" + "=" * 60)
    print("‚úÖ PROMPT ENGINEERING SYSTEM TEST COMPLETE")
    print("=" * 60)
    print("\nüìä Prompt Engineering Component Coverage:")
    print("  ‚úÖ Design systematic prompting strategies (8 templates)")
    print("  ‚úÖ Implement context management (token limits, prioritization)")
    print("  ‚úÖ Create specialized user interaction flows (5 flow types)")
    print("  ‚úÖ Handle edge cases and errors gracefully (6 edge cases)")