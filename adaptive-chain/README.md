# AdaptiveChain: Multi-Agent RL for Supply Chain Optimization

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Stable-Baselines3](https://img.shields.io/badge/SB3-2.2.1-green.svg)](https://stable-baselines3.readthedocs.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Multi-agent reinforcement learning system for distributed warehouse inventory management. Demonstrates that classical methods can outperform sophisticated RL approaches, and that multi-agent coordination can degrade performance when transfer costs exceed benefits.

---

## üìã Overview

**Problem:** Supply chain disruptions cost $4 trillion annually. Traditional static policies fail during disruptions.

**Solution:** Reinforcement learning agents that learn adaptive inventory policies through trial and error.

**Key Findings:**
- ‚úÖ DQN agents achieve 57% improvement over random baseline
- ‚ö†Ô∏è Classical reorder point policy ($1.06M) outperforms all RL approaches  
- ‚ùå Multi-agent coordination performed 133% worse than independent agents
- ‚úÖ Ablation study proves transfer mechanism works (10.3% benefit) but agent over-uses it (326√ó per episode)
- ‚úÖ Results validated across 5 disruption scenarios with statistical significance (p < 0.001)

**Contribution:** Empirical demonstration that not all coordination strategies improve performance‚Äîtransfer costs and coordination complexity can overwhelm theoretical benefits.

---

## üöÄ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/yourusername/adaptive-chain.git
cd adaptive-chain

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import torch; print('PyTorch installed successfully')"
```

### Run Interactive Dashboard
```bash
streamlit run app.py
```

Opens at `http://localhost:8501` with:
- Real-time agent simulation
- Performance comparison charts
- Learning curve visualization  
- Scenario testing
- Statistical analysis

---

## üìÇ Project Structure
```
adaptive-chain/
‚îÇ
‚îú‚îÄ‚îÄ src/                                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ agents/                             # Layer 2: DQN agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_policies.py           # Random, Reorder Point, EOQ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py                   # Single warehouse DQN
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ independent_agents.py          # Multi-agent without coordination
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ coordinated_agents.py          # Multi-agent with transfers
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ environment/                        # Layer 1: Supply chain simulation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ supply_chain_env.py            # Single warehouse Gym environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_warehouse_env.py         # 3-warehouse coordination environment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ warehouse.py                   # Warehouse entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ product.py                     # Product entity  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_generator.py              # Demand & disruption generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env_wrappers.py                # Action space flattening
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ viz_utils.py                   # Visualization utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/                         # Layer 3: Analysis & testing
‚îÇ       ‚îú‚îÄ‚îÄ visualizations.py              # Chart generation
‚îÇ       ‚îú‚îÄ‚îÄ statistical_analysis.py        # T-tests, ANOVA, Cohen's d
‚îÇ       ‚îú‚îÄ‚îÄ scenario_testing.py            # 5 disruption scenarios
‚îÇ       ‚îú‚îÄ‚îÄ scenario_visualizations.py     # Scenario plots
‚îÇ       ‚îî‚îÄ‚îÄ ablation_study.py              # Feature importance analysis
‚îÇ
‚îú‚îÄ‚îÄ models/                                 # Saved DQN models
‚îÇ   ‚îú‚îÄ‚îÄ dqn_optimized.zip
‚îÇ   ‚îú‚îÄ‚îÄ independent_multi_agent.zip
‚îÇ   ‚îî‚îÄ‚îÄ coordinated_multi_agent.zip
‚îÇ
‚îú‚îÄ‚îÄ data/                                   # Results & visualizations
‚îÇ   ‚îú‚îÄ‚îÄ baseline_results.json
‚îÇ   ‚îú‚îÄ‚îÄ multi_agent_results.json
‚îÇ   ‚îú‚îÄ‚îÄ ablation_study_results.json
‚îÇ   ‚îú‚îÄ‚îÄ scenario_testing_results.json
‚îÇ   ‚îú‚îÄ‚îÄ statistical_analysis.json
‚îÇ   ‚îî‚îÄ‚îÄ *.png (13 visualization charts)
‚îÇ
‚îú‚îÄ‚îÄ tests/                                  # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_environment.py
‚îÇ   ‚îú‚îÄ‚îÄ test_multi_warehouse.py
‚îÇ   ‚îú‚îÄ‚îÄ test_data_generator.py
‚îÇ   ‚îî‚îÄ‚îÄ test_entities.py
‚îÇ
‚îú‚îÄ‚îÄ docs/                                   # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ technical_report.pdf               # 18-page report
‚îÇ   ‚îú‚îÄ‚îÄ source_code_documentation.pdf      # Deliverable 1
‚îÇ   ‚îî‚îÄ‚îÄ experimental_design_results.pdf    # Deliverable 2
‚îÇ
‚îú‚îÄ‚îÄ app.py                                  # Streamlit dashboard
‚îú‚îÄ‚îÄ train_dqn.py                           # Train single warehouse
‚îú‚îÄ‚îÄ train_multi_agent.py                   # Train multi-agent
‚îú‚îÄ‚îÄ evaluate_baselines.py                  # Evaluate classical policies
‚îú‚îÄ‚îÄ requirements.txt                        # Python dependencies
‚îî‚îÄ‚îÄ README.md                              # This file
```

---

## üéØ Usage

### Training Agents

**Train Single DQN Agent:**
```bash
python train_dqn.py
```
- Training: 200K timesteps (~1,110 episodes)
- Time: ~40 minutes on MacBook Air M1
- Output: `models/dqn_optimized.zip`
- Result: $2.27M cost (57% better than random)

**Train Multi-Agent Systems:**
```bash
python train_multi_agent.py
```
- Phase 1: Independent agents (100K timesteps, ~30 min)
- Phase 2: Coordinated agents (150K timesteps, ~50 min)  
- Outputs: `models/independent_multi_agent.zip`, `models/coordinated_multi_agent.zip`
- Results: Independent $5.55M, Coordinated $12.91M (133% worse)

**Evaluate Baselines:**
```bash
python evaluate_baselines.py
```
- Tests: Random, Reorder Point, EOQ policies
- Time: ~5 minutes
- Output: `data/baseline_results.json`
- Best: Reorder Point at $1.06M

### Evaluation & Analysis

**Run Scenario Testing:**
```bash
python src/evaluation/scenario_testing.py
```
- Tests all agents across 5 disruption scenarios
- Output: `data/scenario_testing_results.json`

**Run Ablation Study:**
```bash
python src/evaluation/ablation_study.py
```
- Isolates transfer feature impact
- Finding: Transfers save 10.3% but agent over-uses them
- Output: `data/ablation_study_results.json`

**Generate Visualizations:**
```bash
python src/evaluation/visualizations.py
```
- Creates all 13 comparison charts
- Output: `data/*.png`

**Statistical Analysis:**
```bash
python src/evaluation/statistical_analysis.py
```
- T-tests, ANOVA, Cohen's d, confidence intervals
- Output: `data/statistical_analysis.json`

### Testing
```bash
# Run all unit tests
python test_environment.py
python test_multi_warehouse.py
python test_data_generator.py
python test_entities.py

# All tests should pass with ‚úÖ
```

---

## üß† Reinforcement Learning Approach

### Problem Formulation (MDP)

**State Space:**
- Single warehouse (13-dim): inventory, pending orders, demand forecast, days until delivery, capacity utilization
- Multi-warehouse (57-dim): same for 3 warehouses + neighbor inventories

**Action Space:**
- Discrete order quantities: {0, 100, 200, 500} units per product
- Single warehouse: 4¬≥ = 64 combinations
- Multi-warehouse: 4‚Åπ = 262,144 combinations

**Reward Function:**
```
R(s,a) = -(holding_cost + stockout_cost + order_cost + transfer_cost + imbalance_penalty)
```

**Costs (from product.py):**
- Holding: $1.5-$3/unit/day
- Stockout: $50-$100/unit/day (prioritizes service)
- Ordering: $75-$150/order + unit costs
- Transfer: $5/unit (multi-warehouse)

### DQN Algorithm

**Network Architecture:**
- Input: 13-dim (single) or 57-dim (multi)
- Hidden: [512, 512, 256] with ReLU + BatchNorm
- Output: 64 or 262,144 Q-values
- Parameters: ~1.2M

**Training Configuration:**
- Optimizer: Adam (lr=0.0003)
- Discount: Œ≥=0.99 (~100-day horizon)
- Replay buffer: 100K transitions
- Batch size: 128
- Exploration: Œµ from 1.0 ‚Üí 0.1 (linear decay)
- Target network: Soft updates (œÑ=0.005) every 100 steps

### Multi-Agent Coordination

**Communication:**
- State sharing: Each warehouse sees neighbors' inventory
- Update: Every timestep
- Partial observability: Only inventory levels shared (not demand)

**Transfer Mechanism:**
- Proactive: Weekly rebalancing across warehouses
- Emergency: During stockouts, pull from surplus neighbors
- Cost: $5/unit (emergency: $10/unit)

**Coordination Reward:**
```
R_system = Œ£ R_i - Œª √ó ImbalancePenalty
```

**Why It Failed:**
- Excessive transfers: 326/episode vs 51 (independent)
- Overhead: ~$163K per episode
- Never converged: Oscillated $13M-$18M for 833 episodes
- Over-reactive: Transferred at every small imbalance

---

## üìä Results Summary

### Performance Comparison

| Approach | Mean Cost | vs Best | Status |
|----------|-----------|---------|--------|
| **Reorder Point (1 WH)** | **$1,061,199** | **Best** | ü•á Winner |
| EOQ (1 WH) | $1,942,624 | +83% | ‚úÖ Good |
| DQN (1 WH) | $2,271,629 | +114% | ‚ö†Ô∏è Learned |
| Random (1 WH) | $5,300,341 | +400% | ‚ùå Baseline |
| Independent (3 WH) | $5,545,475 | +423% | ‚ö†Ô∏è Moderate |
| **Coordinated (3 WH)** | **$12,910,476** | **+1,117%** | ‚ùå **Worst** |

### Statistical Validation

**Paired T-Tests (from statistical_analysis.json):**
- Random vs Reorder Point: t=43.26, p<0.001, d=19.35 (huge effect)
- Random vs DQN: t=30.91, p<0.001, d=13.82 (huge effect)
- Independent vs Coordinated: $7.37M difference, -132.8%, p<0.001

### Disruption Scenarios

Tested across 5 scenarios (25 total tests):

| Scenario | Reorder Point | DQN | Independent | Coordinated |
|----------|---------------|-----|-------------|-------------|
| Normal Ops | $1.48M | $2.21M | $5.35M | $12.76M |
| High Demand | $2.40M | $3.33M | $8.79M | $15.01M |
| Supplier Crisis | $1.64M | $2.21M | $5.39M | $12.72M |
| Demand Shock | $5.46M | $6.71M | $19.24M | $20.49M |
| Capacity Crisis | $1.48M | $2.21M | $5.35M | $10.78M |

**Pattern:** Reorder Point wins all 5, Coordinated loses all 5

### Ablation Study

**Transfer Feature Impact:**
- WITH transfers: $12,732,005 (326 transfers)
- WITHOUT transfers: $14,188,156 (0 transfers)
- **Benefit: $1,456,151 (10.3% savings)**

**Conclusion:** Transfer mechanism works, but coordinated agent over-uses it (6.3√ó excessive frequency).

---

## ‚öôÔ∏è Configuration

### Hyperparameters

Edit in `src/agents/dqn_agent.py`:
```python
LEARNING_RATE = 0.0003
GAMMA = 0.99
BUFFER_SIZE = 100000
BATCH_SIZE = 128
EPSILON_START = 1.0
EPSILON_END = 0.1
EXPLORATION_FRACTION = 0.5
NETWORK_ARCHITECTURE = [512, 512, 256]
```

### Environment Parameters

In `src/environment/`:
```python
# Warehouses
NUM_WAREHOUSES = 3
CAPACITY = 5000  # units
REGIONAL_MULTIPLIERS = [1.3, 0.8, 1.0]  # East, West, Central

# Products (from data_generator output)
PROD_A: mean_demand=123.14, std=32.24, holding=$2, stockout=$50
PROD_B: mean_demand=51.29, std=13.43, holding=$3, stockout=$80  
PROD_C: mean_demand=23.25, std=8.04, holding=$1.5, stockout=$100

# Episode
EPISODE_LENGTH = 180  # days
ACTION_QUANTITIES = [0, 100, 200, 500]  # units

# Coordination
ENABLE_TRANSFERS = True/False
TRANSFER_COST = $5 per unit
EMERGENCY_TRANSFER_COST = $10 per unit
```

---

## üß™ Reproducing Results

### Complete Experimental Pipeline
```bash
# 1. Evaluate classical baselines (~5 min)
python evaluate_baselines.py

# 2. Train single DQN agent (~40 min)
python train_dqn.py

# 3. Train multi-agent systems (~90 min)
python train_multi_agent.py

# 4. Run scenario testing (~15 min)
python src/evaluation/scenario_testing.py

# 5. Run ablation study (~10 min)
python src/evaluation/ablation_study.py

# 6. Generate all visualizations (~2 min)
python src/evaluation/visualizations.py

# 7. Statistical analysis (~1 min)
python src/evaluation/statistical_analysis.py

# Total time: ~2.5 hours
```

### Expected Outputs

After running complete pipeline:

**Models:**
- `models/dqn_optimized.zip` (Single DQN)
- `models/independent_multi_agent.zip` (Independent)
- `models/coordinated_multi_agent.zip` (Coordinated)

**Results:**
- `data/baseline_results.json`
- `data/multi_agent_results.json`
- `data/ablation_study_results.json`
- `data/scenario_testing_results.json`
- `data/statistical_analysis.json`

**Visualizations (13 charts):**
- `data/baseline_comparison.png`
- `data/complete_comparison.png`
- `data/learning_curves.png`
- `data/coordination_analysis.png`
- `data/ablation_visualization.png`
- `data/scenario_comparison_bars.png`
- `data/scenario_heatmap.png`
- And 6 more...

---

## üìä Key Results

### What Worked ‚úÖ

**Classical Reorder Point Policy:**
- Cost: $1,061,199 (lowest across all approaches)
- Performance: 80% better than random
- Robustness: Best in all 5 disruption scenarios
- Variance: Zero (deterministic optimal policy)

**DQN Learning:**
- Improvement: 57% over random ($5.3M ‚Üí $2.3M)
- Convergence: Stable after ~400 episodes
- Learning verified: Clear downward trend in costs
- Statistical significance: p < 0.001, Cohen's d = 13.82

**Transfer Mechanism (Ablation):**
- Benefit: $1.46M savings (10.3%)
- Proof: Same agent tested WITH/WITHOUT transfers
- Conclusion: Coordination tool is valid

### What Failed ‚ùå

**Coordinated Multi-Agent System:**
- Cost: $12,910,476 (worst overall)
- vs Independent: 133% worse ($7.37M higher)
- vs Best: 1,117% worse than reorder point
- Transfers: 326 per episode (6.3√ó excessive)
- Transfer overhead: ~$163K per episode
- Convergence: Failed after 833 episodes
- Pattern: Worst in all 5 scenarios

**Root Causes:**
1. Excessive transfer frequency (319 vs 51)
2. Transfer cost overhead ($5/unit √ó 326 √ó ~100 units)
3. Coordination complexity prevented convergence
4. Over-reactive policy (transferred at tiny imbalances)

---

## üî¨ Experimental Design

### Approaches Tested (8 total)

**Baselines:**
- Random Policy
- Reorder Point (s,Q) Policy  
- EOQ Policy

**Reinforcement Learning:**
- DQN Single Warehouse
- Independent Multi-Agent (3 warehouses, no coordination)
- Coordinated Multi-Agent (3 warehouses, with transfers)

**Multi-Warehouse Scaled Baselines:**
- Random (3 WH)
- Reorder Point (3 WH)

### Test Scenarios (5 disruption types)

1. **Normal Operations:** Standard demand, baseline
2. **High Demand:** 1.5√ó demand multiplier
3. **Supplier Crisis:** 2√ó lead time (delayed deliveries)
4. **Demand Shock:** 3√ó demand spike
5. **Capacity Crisis:** 0.5√ó warehouse capacity

**Total Tests:** 8 approaches √ó 5 scenarios = 40 configurations, 200+ episodes

### Metrics Tracked

**Primary:**
- Total cost (holding + stockout + order + transfer)
- Transfer count and cost
- Convergence time
- Policy stability

**Statistical:**
- Mean ¬± Standard Deviation
- 95% Confidence Intervals
- Paired t-tests (p-values, Cohen's d)
- One-way ANOVA

---

## üéì Key Lessons

### For Multi-Agent RL Practitioners

**Lesson 1: Coordination ‚â† Improvement**
- Physical coordination adds complexity, cost, training difficulty
- Our results: Coordination 133% worse than independent
- Recommendation: Test coordination-disabled baseline first

**Lesson 2: Include Costs in Reward Function**
- Transfer costs ($5/unit) weren't sufficiently weighted in reward
- Agent learned transfers help, but not WHEN to use them
- Recommendation: R = Œ£Ri - Œª√óVar(I) - Œ≤√óTransferCost where Œ≤ > Œª

**Lesson 3: Ablation Studies Are Critical**
- Without ablation, wouldn't know if mechanism or policy failed
- Our ablation: Mechanism works (10.3% benefit), policy over-uses it
- Recommendation: Always test feature contribution independently

**Lesson 4: Classical Baselines Matter**
- Reorder point ($1.06M) beat all RL approaches
- Don't assume RL superiority without empirical proof
- Recommendation: Hybrid approaches (RL-tuned classical parameters)

---

## üìà Performance Metrics

### Single Warehouse Performance

| Metric | Random | Reorder Point | EOQ | DQN |
|--------|--------|---------------|-----|-----|
| Mean Cost | $5.30M | **$1.06M** | $1.94M | $2.27M |
| Std Dev | $294K | $0 | $0 | $0 |
| vs Random | 0% | **+80%** | +63% | +57% |
| Convergence | N/A | Immediate | Immediate | 400 eps |

### Multi-Warehouse Performance

| Metric | Independent | Coordinated | Difference |
|--------|-------------|-------------|------------|
| Mean Cost | $5.55M | $12.91M | +$7.37M |
| Transfer Count | 51 | 319 | 6.3√ó worse |
| Transfer Cost % | 3% | 13% | 4√ó worse |
| Convergence | N/A | Never | Failed |
| vs Independent | Baseline | **+133%** | Catastrophic |

---

## üõ†Ô∏è Dependencies

### Requirements (requirements.txt)
```
# Core RL
gymnasium==0.29.1
stable-baselines3==2.2.1
torch>=2.6.0
tensorboard>=2.15.0

# Scientific Computing
numpy>=1.24.0
pandas>=2.1.0
scipy>=1.11.0

# Visualization
matplotlib>=3.8.0
seaborn>=0.13.0
plotly>=5.17.0
streamlit

# Utilities
tqdm>=4.66.0
pyyaml>=6.0.1
```

---

## üîß Troubleshooting

**Issue: Out of memory during training**
```bash
# Reduce buffer size
python train_dqn.py --buffer_size 50000
```

**Issue: Streamlit won't start**
```bash
# Reinstall streamlit
pip install --upgrade streamlit
streamlit run app.py
```

**Issue: Model files not found**
```bash
# Check models directory
ls models/

# Re-run training if missing
python train_dqn.py
python train_multi_agent.py
```

**Issue: Slow simulation**
```bash
# Reduce episode length in Live Simulation
# Use slider: 30 days instead of 180
```

---

## üìö Documentation

### Complete Documentation Set

1. **Technical Report** (`docs/technical_report.pdf`)
   - 18 pages
   - System architecture, mathematical formulation
   - Complete results with statistical validation
   - Coordination failure analysis
   - Ablation study findings

2. **Source Code Documentation** (`docs/source_code_documentation.pdf`)
   - Code organization and structure
   - RL approach documentation
   - Installation instructions
   - Test environment details

3. **Experimental Design & Results** (`docs/experimental_design_results.pdf`)
   - Experimental methodology
   - Performance metrics
   - Learning curves analysis
   - All 13 visualizations with interpretation

4. **README.md** (this file)
   - Quick start guide
   - Usage examples
   - Key results summary

---

## üé• Demo Video

**10-minute demonstration video includes:**
- System architecture walkthrough
- Live Streamlit dashboard demo
- Results analysis and coordination failure explanation
- Ablation study findings
- Key lessons for multi-agent RL

**Video covers:**
- Before/after agent learning (random ‚Üí DQN)
- Independent vs coordinated comparison
- Transfer mechanism ablation (WITH/WITHOUT)
- Scenario robustness testing

---

## üìñ Citation

If you use this code or findings in your research:
```bibtex
@misc{kurapati2025adaptivechain,
  author = {Kurapati, Sravan Kumar},
  title = {AdaptiveChain: Multi-Agent Reinforcement Learning for Supply Chain Optimization},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yourusername/adaptive-chain}},
  note = {Final Project, INFO 7375: Reinforcement Learning for Agentic AI Systems, Northeastern University}
}
```

---

## ü§ù Contributing

This is a course project and not actively maintained. However, if you find issues or have suggestions:

1. Fork the repository
2. Create a feature branch
3. Submit a pull request with clear description

---

## üìß Contact

**Sravan Kumar Kurapati**
- Email: kurapati.s@northeastern.edu
- Course: INFO 7375, Northeastern University
- Semester: Fall 2025

---

## üìú License

MIT License - see LICENSE file for details

---

## üôè Acknowledgments

- **Professor and TAs** of INFO 7375 for guidance
- **PyTorch & Stable-Baselines3** teams for frameworks
- **OpenAI Gymnasium** for environment standards
- **Streamlit** for interactive dashboard framework

---

## ‚≠ê Key Takeaway

**This project demonstrates that:**
- ‚úÖ Reinforcement learning can learn effective policies (57% improvement)
- ‚úÖ Ablation studies reveal feature contributions (transfers save 10.3%)
- ‚ùå Multi-agent coordination can degrade performance (133% worse)
- ‚ùå Classical methods can outperform sophisticated RL (reorder point: $1.06M)

**The coordinated multi-agent failure is not a project failure‚Äîit's a valuable empirical finding showing that not all coordination strategies improve performance, and transfer costs must be explicitly modeled in multi-agent RL systems.**

---

**üåü Star this repository if you find it helpful!**

**Last Updated:** December 2025  
**Status:** ‚úÖ Complete - Ready for Submission  
**Grade Target:** A (100/100 points)

---