# Deep Q-Learning for Atari Kangaroo

**Course:** INFO 7375 - Fine-Tuning Large Language Models  
**Author:** Sravan Kumar Kurapati  
**Game:** Kangaroo (ALE/Kangaroo-v5)  
**Date:** November 2025

## ğŸ® Project Overview

This project implements a Deep Q-Network (DQN) agent to play the Atari Kangaroo game using reinforcement learning. The implementation features experience replay, frame preprocessing, epsilon-greedy exploration, and comprehensive experimentation with various hyperparameters.

## ğŸ† Key Results

- **Best Performance:** 188.8 mean reward (Experiment 5: epsilon_decay=0.99)
- **8 Systematic Experiments** testing learning rates, gamma values, and exploration strategies
- **Boltzmann Exploration** experiment showing limitations for this environment
- **Professional visualizations** with training curves and performance analysis

## ğŸ“ Project Structure

```
kangaroo-dqn/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ dqn_agent.py                      # Core DQN implementation
â”œâ”€â”€ experiment_configs.py             # Configuration for all experiments
â”œâ”€â”€ run_single_experiment.py          # Experiment orchestration
â”œâ”€â”€ run_boltzmann_experiment.py       # Boltzmann exploration variant
â”œâ”€â”€ docs/                             # Complete documentation
â”‚   â”œâ”€â”€ 01_baseline_performance.md
â”‚   â”œâ”€â”€ 02_environment_analysis.md
â”‚   â”œâ”€â”€ 03_reward_structure.md
â”‚   â”œâ”€â”€ 04_bellman_parameters.md
â”‚   â”œâ”€â”€ 05_policy_exploration.md
â”‚   â”œâ”€â”€ 06_exploration_parameters.md
â”‚   â”œâ”€â”€ 07_performance_metrics.md
â”‚   â”œâ”€â”€ 08-14_theoretical_questions.md
â”‚   â””â”€â”€ 15-18_code_documentation.md
â””â”€â”€ experiment_results/               # Results from all experiments
    â”œâ”€â”€ exp1_baseline/
    â”œâ”€â”€ exp2_lr_0001/
    â”œâ”€â”€ exp3_lr_001/
    â””â”€â”€ ... (9 experiments total)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- macOS (tested on M1 MacBook Air) or Linux
- 8GB+ RAM recommended

### Installation

```bash
# Clone repository
git clone <your-repo-url>
cd kangaroo-dqn

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Atari ROMs
pip install gymnasium[atari,accept-rom-license]
```

### Running Experiments

```bash
# Run baseline experiment
python run_single_experiment.py --experiment 1

# Run all experiments
for i in {1..8}; do
    python run_single_experiment.py --experiment $i
done

# Run Boltzmann exploration experiment
python run_boltzmann_experiment.py
```

## ğŸ§ª Experimental Results

### Experiment Configuration Summary

| Exp | Learning Rate | Gamma | Epsilon Decay | Mean Reward | Outcome |
|-----|---------------|-------|---------------|-------------|---------|
| 1   | 0.00025       | 0.99  | 0.995         | 178.6       | Baseline |
| 2   | 0.0001        | 0.99  | 0.995         | 176.2       | Slower learning |
| 3   | 0.001         | 0.99  | 0.995         | 165.4       | Unstable |
| 4   | 0.00025       | 0.8   | 0.995         | 162.8       | Shortsighted |
| 5   | 0.00025       | 0.99  | 0.99          | **188.8**   | **Best** |
| 6   | 0.00025       | 0.99  | 0.998         | 170.3       | Too conservative |
| 7   | 0.00025       | 0.9   | 0.995         | 172.1       | Moderate |
| 8   | 0.00025       | 0.95  | 0.995         | 174.5       | Balanced |
| 9   | 0.00025       | 0.99  | Boltzmann     | 44.0        | Failed |

### Key Findings

1. **Slower epsilon decay (0.99) performs best** - Allows more exploration during training
2. **Higher gamma (0.99) is crucial** - Long-term planning essential for Kangaroo gameplay
3. **Boltzmann exploration fails** - Unsuitable for this environment's action space
4. **Learning rate sensitivity** - 0.00025 provides optimal stability vs. speed

## ğŸ¯ Assignment Requirements

All 18 assignment requirements are fully documented:

### Experimental Results (35 points)
- âœ… Section 1: Baseline Performance
- âœ… Section 2: Environment Analysis  
- âœ… Section 3: Reward Structure
- âœ… Section 4: Bellman Parameters (Î±, Î³)
- âœ… Section 5: Policy Exploration
- âœ… Section 6: Exploration Parameters (Îµ)
- âœ… Section 7: Performance Metrics

### Theoretical Questions (35 points)
- âœ… Section 8: Q-Learning Classification
- âœ… Section 9: Q-Learning vs. LLM Agents
- âœ… Section 10: Bellman Equation Concepts
- âœ… Section 11: RL for LLM Agents
- âœ… Section 12: Planning in RL vs. LLM
- âœ… Section 13: Q-Learning Algorithm
- âœ… Section 14: LLM Agent Integration

### Code Documentation (30 points)
- âœ… Section 15: Code Attribution
- âœ… Section 16: Code Clarity
- âœ… Section 17: Licensing
- âœ… Section 18: Professionalism

See `/docs` directory for complete documentation.

## ğŸ§  Technical Implementation

### DQN Architecture

```
Input: 84x84x4 grayscale frames (stacked)
   â†“
Conv2D(32, 8x8, stride=4) + ReLU
   â†“
Conv2D(64, 4x4, stride=2) + ReLU
   â†“
Conv2D(64, 3x3, stride=1) + ReLU
   â†“
Flatten â†’ Dense(512) + ReLU
   â†“
Output: Dense(18) [Q-values for each action]
```

### Key Features

- **Experience Replay**: 10,000 transition buffer
- **Frame Preprocessing**: 84x84 grayscale, frame stacking (4 frames)
- **Target Network**: Updated every 1000 steps
- **Epsilon-Greedy Exploration**: Start=1.0, min=0.01
- **Batch Learning**: Size 32
- **Huber Loss**: Robust to outliers

### Hyperparameters

```python
learning_rate = 0.00025      # Adam optimizer
gamma = 0.99                 # Discount factor
epsilon_start = 1.0          # Initial exploration
epsilon_min = 0.01           # Minimum exploration
epsilon_decay = 0.995        # Decay per episode
batch_size = 32              # Experience replay batch
memory_size = 10000          # Replay buffer capacity
target_update_freq = 1000    # Target network updates
max_steps = 500              # Steps per episode
num_episodes = 1000          # Training episodes
```

## ğŸ“Š Performance Visualization

Training curves, reward distributions, and comparative analyses are available in `experiment_results/` for each experiment.

## ğŸ”¬ Code Attribution

- **Original Code (60%)**: Experiment infrastructure, configurations, orchestration, analysis
- **Adapted Code (40%)**: DQN core architecture based on Mnih et al. (2015) and OpenAI Baselines (MIT License)

Full attribution details in `/docs/15_code_attribution.md`

## ğŸ“œ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

### Third-Party Licenses
- OpenAI Baselines: MIT License
- Gymnasium: MIT License
- ALE (Arcade Learning Environment): GPL-2.0 (runtime only)

## ğŸ“ Academic Context

This project fulfills the requirements for the INFO 7375 assignment on Deep Q-Learning. All code follows academic integrity guidelines with proper attribution and licensing.

### Assignment Adaptations

The assignment suggested parameters (learning_rate=0.7, max_steps=99) are appropriate for tabular Q-learning but not for Deep Q-Networks. This implementation follows established DQN literature (Mnih et al., 2015) using:
- learning_rate=0.00025 (neural network standard)
- max_steps=500 (sufficient for Kangaroo gameplay)
- 1000 episodes (computational feasibility)

## ğŸ“š References

1. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." *Nature*, 518(7540), 529-533.
2. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT Press.
3. OpenAI Baselines: https://github.com/openai/baselines
4. Gymnasium Documentation: https://gymnasium.farama.org/
5. ALE Kangaroo Environment: https://ale.farama.org/environments/kangaroo/

## ğŸ¤ Contributing

This is an academic project. For questions or suggestions, please contact the author.

## ğŸ“§ Contact

**Sravan Kumar Kurapati**  
Course: INFO 7375 - Fine-Tuning Large Language Models  
Northeastern University

---

**Note**: This implementation demonstrates Deep Q-Learning for educational purposes. The agent achieves reasonable performance (188.8 mean reward) though professional-grade implementations may achieve higher scores through additional optimizations.